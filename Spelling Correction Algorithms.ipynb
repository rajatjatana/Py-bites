{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spelling Correction Algorithms\n",
    "\n",
    "The aim of this notebook is to illustrate spelling correction algorithms. \n",
    "\n",
    "\n",
    "### Overview: \n",
    "\n",
    "Many a times we have to deal with the text data and it is very common to find the spelling mistakes. Spelling mistakes can mean loss of information in such cases. But, thanks to simple spelling correction aglorithms like this one, we can correct the spelling mistakes with good accuracy. \n",
    "\n",
    "A use-case: While analysing the customer reviews for various products, we might encounter many texts with slangs or improper spellings. This algorithm can be used to treat the text before putting it through any analysis like sentiment analysis, topic modelling, etc. \n",
    "\n",
    "\n",
    "![title](Data/spelling_mistakes_meme.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plan of attack: \n",
    "\n",
    "I first plan to load the dataset that I shall be using to create a reference dictionary for correcting the spellings. Larger it is the better it is. Then, we will create the algorithm to correct the spellings.\n",
    "\n",
    "#### Credits\n",
    "The post is inspired by a post on Peter Norvig's [blog.](http://www.norvig.com/). The algorithm and some code snippets is borrowed from his blog.\n",
    "\n",
    "Let's get started... \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dependencies : \n",
    "import re  # For regular expressions \n",
    "import urllib\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "text= urllib.request.urlopen('https://raw.githubusercontent.com/norvig/pytudes/master/data/text/big.txt').read().decode('utf-8', 'strict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6488666\n"
     ]
    }
   ],
   "source": [
    "print(len(text)) # such a large text!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's treat the text above and get rid of any punctuations numbers and special characters.Also, we will convert the text into lowercase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_lowercase(text): \n",
    "    '''Convert the text into lowercase '''\n",
    "    return( re.findall('[a-z]+' , text.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'project', 'gutenberg', 'ebook', 'of', 'the', 'adventures', 'of', 'sherlock', 'holmes', 'by', 'sir', 'arthur', 'conan', 'doyle', 'in', 'our', 'series', 'by', 'sir']\n"
     ]
    }
   ],
   "source": [
    "words = tokenize_and_lowercase(text)\n",
    "\n",
    "# Here are the first few of them: \n",
    "\n",
    "print(words[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets take the sample from this and join them:\n",
    "import random\n",
    "def sample_n(bag , n): \n",
    "    ''' Select n words at a random'''\n",
    "    return(' '.join(random.choice(bag) for i in range(n)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i countess bridge margins hard the third at general two has drop was pipes to meaning more begins bank measures'"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_n(words , 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bag of words: \n",
    "\n",
    "One way to represent a bag of words is using dictionary style - \"Counter\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 5810), ('and', 3088), ('i', 3038), ('to', 2823), ('of', 2778), ('a', 2701), ('in', 1823), ('that', 1767), ('it', 1749), ('you', 1572)]\n"
     ]
    }
   ],
   "source": [
    "counts= Counter(words)\n",
    "print(bag.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spelling Correction :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me take a jab at explaining the algorithm: \n",
    "\n",
    "**Intuition:**  \n",
    "For every word, try to find out the words which are near to that word and are more likely to be the correct ones. \n",
    "\n",
    "That might sound little confusing. But let's try to eliminate the confusion. \n",
    "\n",
    "Say, we have a word - *wird*. Here, the words that are near to this can be wirdh', 'wirdw', 'jird'.....'weird', ....., etc. The one which is most likely to be found in our corpus and is nearest to it is 'weird' (which is also the right word). Our algorithm will pick it up in that case.\n",
    "\n",
    "**How do we define near?**\n",
    "\n",
    "We use the following 4 ways to extract the possible combinations of near words- \n",
    "\n",
    "| Method | Examples | \n",
    "| --- | --- | \n",
    "| deletes  | 'ird' ,  'wrd' , 'wid', 'wir'|\n",
    "| transposes  | 'iwrd', 'wrid', 'widr'|\n",
    "| replaces  |   'ird' ,  'wrd' , 'wid', 'wir'|\n",
    "| inserts  |  'aird', 'bird', 'cird', 'dird', 'eird'|\n",
    "\n",
    "We can also put another layer of edits, i.e. extracting the edits of the edits. \n",
    "\n",
    "\n",
    "Step 1 : Define the candidates\n",
    "Step 2 :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Defining all the candidates: \n",
    "def correct(word):\n",
    "    \"Find the best spelling correction for this word.\"\n",
    "    # Prefer edit distance 0, then 1, then 2; otherwise default to word itself.\n",
    "    candidates = (known(edits0(word)) or \n",
    "                  known(edits1(word)) or \n",
    "                  known(edits2(word)) or \n",
    "                  [word])\n",
    "    return max(candidates, key=counts.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def known(words):\n",
    "    \"Return the subset of words that are actually in the dictionary.\"\n",
    "    return {w for w in words if w in counts}\n",
    "\n",
    "# The edit one\n",
    "def edits0(word): \n",
    "    \"Return all strings that are zero edits away from word (i.e., just word itself).\"\n",
    "    return {word}\n",
    "\n",
    "# Edit 2 \n",
    "def edits2(word):\n",
    "    \"Return all strings that are two edits away from this word.\"\n",
    "    return {e2 for e1 in edits1(word) for e2 in edits1(e1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit 1 \n",
    "def edits1(word):\n",
    "    \"Return all strings that are one edit away from this word.\"\n",
    "    pairs      = splits(word)\n",
    "    deletes    = [a+b[1:]           for (a, b) in pairs if b]\n",
    "    transposes = [a+b[1]+b[0]+b[2:] for (a, b) in pairs if len(b) > 1]\n",
    "    replaces   = [a+c+b[1:]         for (a, b) in pairs for c in alphabet if b]\n",
    "    inserts    = [a+c+b             for (a, b) in pairs for c in alphabet]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def splits(word):\n",
    "    \"Return a list of all possible (first, rest) pairs that comprise word.\"\n",
    "    return [(word[:i], word[i:]) \n",
    "            for i in range(len(word)+1)]\n",
    "\n",
    "alphabet = string.ascii_lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 'spelll'),\n",
       " ('s', 'pelll'),\n",
       " ('sp', 'elll'),\n",
       " ('spe', 'lll'),\n",
       " ('spel', 'll'),\n",
       " ('spell', 'l'),\n",
       " ('spelll', '')]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits('spelll')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'wird'}\n"
     ]
    }
   ],
   "source": [
    "print(edits0('wird'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cwird', 'wire', 'wiid', 'lird', 'cird', 'kird', 'wbird', 'wirmd', 'twird', 'wuird', 'wxrd', 'wikrd', 'zwird', 'wircd', 'mird', 'wirld', 'wjrd', 'wiyd', 'wiird', 'hird', 'wirud', 'wirdv', 'wirdw', 'wyrd', 'wijd', 'werd', 'wsrd', 'ward', 'wivd', 'wirb', 'wirpd', 'wiprd', 'wirdp', 'wirvd', 'wrrd', 'wiro', 'wirs', 'widr', 'wied', 'wicd', 'fwird', 'wirw', 'wizd', 'wgird', 'wirdm', 'wrid', 'wiwrd', 'bird', 'wiad', 'wfrd', 'wprd', 'ewird', 'whird', 'wirdd', 'wirdz', 'sird', 'wirdl', 'hwird', 'ywird', 'wirsd', 'jird', 'kwird', 'wgrd', 'rird', 'wirgd', 'wxird', 'wilrd', 'vwird', 'wirdx', 'wirdo', 'wkrd', 'wdird', 'wirqd', 'wifrd', 'wirc', 'wiqrd', 'iwrd', 'wnrd', 'word', 'wibrd', 'widrd', 'wirdn', 'jwird', 'lwird', 'wlird', 'nwird', 'fird', 'wirl', 'oird', 'wiurd', 'wifd', 'wibd', 'wirrd', 'wvird', 'wirad', 'wirm', 'wirbd', 'wiyrd', 'wirzd', 'wimrd', 'wiard', 'wirxd', 'wirda', 'wirdr', 'wrird', 'witrd', 'wired', 'wqrd', 'wirdk', 'wizrd', 'waird', 'wiod', 'wigd', 'wirdf', 'wijrd', 'wirh', 'wirq', 'tird', 'wivrd', 'wiqd', 'bwird', 'wiri', 'xwird', 'wirj', 'wild', 'wirdh', 'wirt', 'dwird', 'wyird', 'woird', 'witd', 'wirdu', 'wtrd', 'wirdb', 'wirds', 'wvrd', 'wiru', 'wirf', 'wiud', 'rwird', 'wira', 'gwird', 'wirfd', 'vird', 'wrd', 'wicrd', 'wirz', 'wiwd', 'mwird', 'wirjd', 'qird', 'dird', 'wnird', 'wiord', 'wixrd', 'swird', 'weird', 'whrd', 'wipd', 'wigrd', 'uird', 'wirdy', 'wurd', 'wirod', 'xird', 'wirv', 'awird', 'uwird', 'wirdj', 'wirr', 'wlrd', 'wjird', 'wirdi', 'wikd', 'wirid', 'wirk', 'wirn', 'owird', 'wwird', 'wirnd', 'wzrd', 'wirdt', 'zird', 'wimd', 'wird', 'pwird', 'wbrd', 'wfird', 'nird', 'wpird', 'wir', 'wirdc', 'wisd', 'wihrd', 'pird', 'wqird', 'wixd', 'aird', 'wirhd', 'wirkd', 'qwird', 'widd', 'wirp', 'wirde', 'wiryd', 'wirdq', 'yird', 'wwrd', 'wihd', 'winrd', 'wdrd', 'eird', 'wirdg', 'wierd', 'wind', 'gird', 'iird', 'wisrd', 'wirtd', 'wirwd', 'wirg', 'wkird', 'iwird', 'wcrd', 'wirx', 'wsird', 'wzird', 'wid', 'wiry', 'wcird', 'wmird', 'ird', 'wmrd', 'wtird'}\n"
     ]
    }
   ],
   "source": [
    "print(edits1('wird'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24254\n"
     ]
    }
   ],
   "source": [
    "print(len(edits2('wird')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_text(text):\n",
    "    \"Correct all the words within a text, returning the corrected text.\"\n",
    "    return re.sub('[a-zA-Z]+', correct_match, text)\n",
    "\n",
    "def correct_match(match):\n",
    "    \"Spell-correct word in match, and preserve proper upper/lower/title case.\"\n",
    "    word = match.group()\n",
    "    return case_of(word)(correct(word.lower()))\n",
    "\n",
    "def case_of(text):\n",
    "    \"Return the case-function appropriate for text: upper, lower, title, or just str.\"\n",
    "    return (str.upper if text.isupper() else\n",
    "            str.lower if text.islower() else\n",
    "            str.title if text.istitle() else\n",
    "            str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to test the results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Please do at make any mistakes'"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_text('Pleaze do nat makk any mestakes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is wrong here? Nothing friend, all is well'"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_text('Whaat is wrong heree? Nothingg friendd, aal is weel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appendix:\n",
    "\n",
    "big.txt : It is a large text body of 6 million words. It is compiled by some texts from gutenberg and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "\n",
    "pairs      = splits(word)\n",
    "deletes    = [a+b[1:]           for (a, b) in pairs if b]\n",
    "transposes = [a+b[1]+b[0]+b[2:] for (a, b) in pairs if len(b) > 1]\n",
    "replaces   = [a+c+b[1:]         for (a, b) in pairs for c in alphabet if b]\n",
    "inserts    = [a+c+b             for (a, b) in pairs for c in alphabet]\n",
    "return set(deletes + transposes + replaces + inserts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td>1</td><td>2</td><td>3</td></tr><tr><td>4</td><td>5</td><td>6</td></tr><tr><td>7</td><td>8</td><td>9</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "data = [[1,2,3],\n",
    "        [4,5,6],\n",
    "        [7,8,9],\n",
    "        ]\n",
    "\n",
    "display(HTML(\n",
    "   '<table><tr>{}</tr></table>'.format(\n",
    "       '</tr><tr>'.join(\n",
    "           '<td>{}</td>'.format('</td><td>'.join(str(_) for _ in row)) for row in data)\n",
    "       )\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Stretch/Untouched | ProbDistribution | Accuracy |\n",
    "| --- | --- | --- |\n",
    "| Stretched | Gaussian | .843 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abcdefghijklmnopqrstuvwxyz'"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.ascii_lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
